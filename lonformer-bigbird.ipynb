{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import gc\ngc.enable()\n\nimport sys\nsys.path.append(\"../input/tez-lib/tez-lib\")\n\nimport os\n\n!nvidia-smi\nimport torch\ntorch.cuda.is_available()\n\nimport numpy as np\nimport pandas as pd\nimport tez\nimport torch\nimport torch.nn as nn\nfrom joblib import Parallel, delayed\nfrom transformers import AutoConfig, AutoModel, AutoTokenizer, BigBirdTokenizer, BigBirdModel\nfrom torch.optim import Adam\nfrom tqdm import tqdm","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-28T19:49:23.200435Z","iopub.execute_input":"2022-04-28T19:49:23.200903Z","iopub.status.idle":"2022-04-28T19:49:30.741692Z","shell.execute_reply.started":"2022-04-28T19:49:23.200817Z","shell.execute_reply":"2022-04-28T19:49:30.740864Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"target_id_map = {\n    \"B-Lead\": 0,\n    \"I-Lead\": 1,\n    \"B-Position\": 0,\n    \"I-Position\": 1,\n    \"B-Evidence\": 0,\n    \"I-Evidence\": 1,\n    \"B-Claim\": 0,\n    \"I-Claim\": 1,\n    \"B-Concluding Statement\": 0,\n    \"I-Concluding Statement\": 1,\n    \"B-Counterclaim\": 0,\n    \"I-Counterclaim\": 1,\n    \"B-Rebuttal\": 0,\n    \"I-Rebuttal\": 1,\n    \"O\": 2,\n    \"PAD\": -100,\n}\n\n\nid_target_map = {v: k for k, v in target_id_map.items()}\n\nclass args1:\n    input_path = \"../input/feedback-prize-2021/\"\n    model = \"../input/longformer4096/allenai--longformer-base-4096.main.e351d9d5da3eed48886f39eed7b64014debe4925/\"\n    #model = \"../input/deberta-base/microsoft--deberta-base.main.7d4c0126b06bd59dccd3e48e467ed11e37b77f3f/\"\n\n    tez_model= \"../input/longformer-trained/\"\n    #tez_model = \"../input/deberta/\"\n    output = \".\"\n    batch_size = 8\n    max_len = 4096","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:49:34.128519Z","iopub.execute_input":"2022-04-28T19:49:34.128807Z","iopub.status.idle":"2022-04-28T19:49:34.135722Z","shell.execute_reply.started":"2022-04-28T19:49:34.128765Z","shell.execute_reply":"2022-04-28T19:49:34.134924Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"class FeedbackDataset:\n    def __init__(self, samples, max_len, tokenizer):\n        self.samples = samples\n        self.max_len = max_len\n        self.tokenizer = tokenizer\n        self.length = len(samples)\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        input_ids = self.samples[idx][\"input_ids\"]\n        # print(input_ids)\n        # print(input_labels)\n\n        # add start token id to the input_ids\n        input_ids = [self.tokenizer.cls_token_id] + input_ids\n\n        if len(input_ids) > self.max_len - 1:\n            input_ids = input_ids[: self.max_len - 1]\n\n        # add end token id to the input_ids\n        input_ids = input_ids + [self.tokenizer.sep_token_id]\n        attention_mask = [1] * len(input_ids)\n\n        # padding_length = self.max_len - len(input_ids)\n        # if padding_length > 0:\n        #     if self.tokenizer.padding_side == \"right\":\n        #         input_ids = input_ids + [self.tokenizer.pad_token_id] * padding_length\n        #         attention_mask = attention_mask + [0] * padding_length\n        #     else:\n        #         input_ids = [self.tokenizer.pad_token_id] * padding_length + input_ids\n        #         attention_mask = [0] * padding_length + attention_mask\n\n        # return {\n        #     \"ids\": torch.tensor(input_ids, dtype=torch.long),\n        #     \"mask\": torch.tensor(attention_mask, dtype=torch.long),\n        # }\n\n        return {\n            \"ids\": input_ids,\n            \"mask\": attention_mask,\n        }","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:49:36.080085Z","iopub.execute_input":"2022-04-28T19:49:36.080654Z","iopub.status.idle":"2022-04-28T19:49:36.089499Z","shell.execute_reply.started":"2022-04-28T19:49:36.080615Z","shell.execute_reply":"2022-04-28T19:49:36.088373Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"class Collate:\n    def __init__(self, tokenizer):\n        self.tokenizer = tokenizer\n\n    def __call__(self, batch):\n        output = dict()\n        output[\"ids\"] = [sample[\"ids\"] for sample in batch]\n        output[\"mask\"] = [sample[\"mask\"] for sample in batch]\n\n        # calculate max token length of this batch\n        batch_max = max([len(ids) for ids in output[\"ids\"]])\n\n        # add padding\n        if self.tokenizer.padding_side == \"right\":\n            output[\"ids\"] = [s + (batch_max - len(s)) * [self.tokenizer.pad_token_id] for s in output[\"ids\"]]\n            output[\"mask\"] = [s + (batch_max - len(s)) * [0] for s in output[\"mask\"]]\n        else:\n            output[\"ids\"] = [(batch_max - len(s)) * [self.tokenizer.pad_token_id] + s for s in output[\"ids\"]]\n            output[\"mask\"] = [(batch_max - len(s)) * [0] + s for s in output[\"mask\"]]\n\n        # convert to tensors\n        output[\"ids\"] = torch.tensor(output[\"ids\"], dtype=torch.long)\n        output[\"mask\"] = torch.tensor(output[\"mask\"], dtype=torch.long)\n\n        return output","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:49:37.639239Z","iopub.execute_input":"2022-04-28T19:49:37.639838Z","iopub.status.idle":"2022-04-28T19:49:37.649697Z","shell.execute_reply.started":"2022-04-28T19:49:37.639781Z","shell.execute_reply":"2022-04-28T19:49:37.648907Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"class FeedbackModel(tez.Model):\n    def __init__(self, model_name, num_labels):\n        super().__init__()\n        self.model_name = model_name\n        self.num_labels = num_labels\n        config = AutoConfig.from_pretrained(model_name)\n\n        hidden_dropout_prob: float = 0.1\n        layer_norm_eps: float = 1e-7\n        config.update(\n            {\n                \"output_hidden_states\": True,\n                \"hidden_dropout_prob\": hidden_dropout_prob,\n                \"layer_norm_eps\": layer_norm_eps,\n                \"add_pooling_layer\": False,\n            }\n        )\n        self.transformer = AutoModel.from_config(config)\n        self.output = nn.Linear(config.hidden_size, self.num_labels)\n\n    def forward(self, ids, mask):\n        transformer_out = self.transformer(ids, mask)\n        sequence_output = transformer_out.last_hidden_state\n        logits = self.output(sequence_output)\n        logits = torch.softmax(logits, dim=-1)\n        return logits, 0, {}","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:49:39.381789Z","iopub.execute_input":"2022-04-28T19:49:39.382071Z","iopub.status.idle":"2022-04-28T19:49:39.390987Z","shell.execute_reply.started":"2022-04-28T19:49:39.382039Z","shell.execute_reply":"2022-04-28T19:49:39.390136Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def _prepare_test_data_helper(args, tokenizer, ids):\n    test_samples = []\n    for idx in ids:\n        filename = os.path.join(args.input_path, \"test\", idx + \".txt\")\n        with open(filename, \"r\") as f:\n            text = f.read()\n\n        encoded_text = tokenizer.encode_plus(\n            text,\n            add_special_tokens=False,\n            return_offsets_mapping=True,\n        )\n        input_ids = encoded_text[\"input_ids\"]\n        offset_mapping = encoded_text[\"offset_mapping\"]\n\n        sample = {\n            \"id\": idx,\n            \"input_ids\": input_ids,\n            \"text\": text,\n            \"offset_mapping\": offset_mapping,\n        }\n\n        test_samples.append(sample)\n    return test_samples\n\n\ndef prepare_test_data(df, tokenizer, args):\n    test_samples = []\n    ids = df[\"id\"].unique()\n    ids_splits = np.array_split(ids, 4)\n    #print(ids)\n#     results = Parallel(n_jobs=4, backend=\"multiprocessing\")(\n#         delayed(_prepare_test_data_helper)(args, tokenizer, idx) for idx in ids_splits\n#     )\n    results = []\n    for idx in ids_splits:\n        results.append(_prepare_test_data_helper(args, tokenizer, idx))\n   #########     \n    for result in results:\n        test_samples.extend(result)\n\n    return test_samples","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:50:03.854870Z","iopub.execute_input":"2022-04-28T19:50:03.855171Z","iopub.status.idle":"2022-04-28T19:50:03.866276Z","shell.execute_reply.started":"2022-04-28T19:50:03.855139Z","shell.execute_reply":"2022-04-28T19:50:03.865267Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"files = os.listdir('../input/feedback-prize-2021/test/')\nIDS = np.array([f.replace('.txt','') for f in files if 'txt' in f])\n\ndf = pd.DataFrame({\"id\": IDS,\"class\": np.nan, \"predictionstring\": np.nan})\n\n#tokenizer = AutoTokenizer.from_pretrained('../input/deberta-base/microsoft--deberta-base.main.7d4c0126b06bd59dccd3e48e467ed11e37b77f3f/')\ntokenizer = AutoTokenizer.from_pretrained('../input/longformer4096/allenai--longformer-base-4096.main.e351d9d5da3eed48886f39eed7b64014debe4925/')\n\ncollate = Collate(tokenizer=tokenizer)\n\ntest_samples = prepare_test_data(df, tokenizer, args1)\n\nraw_preds = []\n# for fold_ in range(10):\n#     current_idx = 0\n#     test_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n    \n#     if fold_ < 5:\n#         model = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\n#         model.load(os.path.join(args1.tez_model, f\"model_{fold_}.bin\"), weights_only=True)\n#         preds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n#     else:\n#         model = FeedbackModel(model_name=args2.model, num_labels=len(target_id_map) - 1)\n#         model.load(os.path.join(args2.tez_model, f\"model_{fold_-5}.bin\"), weights_only=True)\n#         preds_iter = model.predict(test_dataset, batch_size=args2.batch_size, n_jobs=-1, collate_fn=collate)\n\nfold_ = 0\ncurrent_idx = 0\ntest_dataset = FeedbackDataset(test_samples, args1.max_len, tokenizer)\n\nmodel = FeedbackModel(model_name=args1.model, num_labels=len(target_id_map) - 1)\nmodel.load(os.path.join(args1.tez_model, f\"model_{fold_}_longformer.bin\"), weights_only=True)\n#model.load(os.path.join(args1.tez_model, f\"model_{fold_}_deberta.bin\"), weights_only=True)\n\npreds_iter = model.predict(test_dataset, batch_size=args1.batch_size, n_jobs=-1, collate_fn=collate)\n\ncurrent_idx = 0\n\nfor preds in preds_iter:\n    preds = preds.astype(np.float16)\n    # preds = preds / 10\n    if fold_ == 0:\n        raw_preds.append(preds)\n    else:\n        raw_preds[current_idx] += preds\n        current_idx += 1\ntorch.cuda.empty_cache()\ngc.collect()","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:50:06.983281Z","iopub.execute_input":"2022-04-28T19:50:06.983849Z","iopub.status.idle":"2022-04-28T19:50:23.841727Z","shell.execute_reply.started":"2022-04-28T19:50:06.983815Z","shell.execute_reply":"2022-04-28T19:50:23.840953Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"final_preds = []\nfinal_scores = []\n\nfor rp in raw_preds:\n    pred_class = np.argmax(rp, axis=2)\n    pred_scrs = np.max(rp, axis=2)\n    print(pred_class, pred_scrs)\n    for pred, pred_scr in zip(pred_class, pred_scrs):\n        pred = pred.tolist()\n        pred_scr = pred_scr.tolist()\n        final_preds.append(pred)\n        final_scores.append(pred_scr)\n\nfor j in range(len(test_samples)):\n    tt = [id_target_map[p] for p in final_preds[j][1:]]\n    tt_score = final_scores[j][1:]\n    test_samples[j][\"preds\"] = tt\n    test_samples[j][\"pred_scores\"] = tt_score","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:50:40.592123Z","iopub.execute_input":"2022-04-28T19:50:40.592392Z","iopub.status.idle":"2022-04-28T19:50:40.606196Z","shell.execute_reply.started":"2022-04-28T19:50:40.592361Z","shell.execute_reply":"2022-04-28T19:50:40.605240Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def jn(pst, start, end):\n    return \" \".join([str(x) for x in pst[start:end]])\n\n\ndef link_evidence(oof):\n    thresh = 1\n    idu = oof['id'].unique()\n    idc = idu[1]\n    eoof = oof[oof['class'] == \"Evidence\"]\n    neoof = oof[oof['class'] != \"Evidence\"]\n    for thresh2 in range(26,27, 1):\n        retval = []\n        for idv in idu:\n            for c in  ['Lead', 'Position', 'Evidence', 'Claim', 'Concluding Statement',\n                   'Counterclaim', 'Rebuttal']:\n                q = eoof[(eoof['id'] == idv) & (eoof['class'] == c)]\n                if len(q) == 0:\n                    continue\n                pst = []\n                for i,r in q.iterrows():\n                    pst = pst +[-1] + [int(x) for x in r['predictionstring'].split()]\n                start = 1\n                end = 1\n                for i in range(2,len(pst)):\n                    cur = pst[i]\n                    end = i\n                    #if pst[start] == 205:\n                    #   print(cur, pst[start], cur - pst[start])\n                    if (cur == -1 and c != 'Evidence') or ((cur == -1) and ((pst[i+1] > pst[end-1] + thresh) or (pst[i+1] - pst[start] > thresh2))):\n                        retval.append((idv, c, jn(pst, start, end)))\n                        start = i + 1\n                v = (idv, c, jn(pst, start, end+1))\n                #print(v)\n                retval.append(v)\n        roof = pd.DataFrame(retval, columns = ['id', 'class', 'predictionstring']) \n        roof = roof.merge(neoof, how='outer')\n        return roof","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:50:42.791233Z","iopub.execute_input":"2022-04-28T19:50:42.791811Z","iopub.status.idle":"2022-04-28T19:50:42.804788Z","shell.execute_reply.started":"2022-04-28T19:50:42.791768Z","shell.execute_reply":"2022-04-28T19:50:42.804102Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"proba_thresh = {\n    \"Lead\": 0.7,\n    \"Position\": 0.55,\n    \"Evidence\": 0.65,\n    \"Claim\": 0.55,\n    \"Concluding Statement\": 0.7,\n    \"Counterclaim\": 0.5,\n    \"Rebuttal\": 0.55,\n}\n\nmin_thresh = {\n    \"Lead\": 9,\n    \"Position\": 5,\n    \"Evidence\": 14,\n    \"Claim\": 3,\n    \"Concluding Statement\": 11,\n    \"Counterclaim\": 6,\n    \"Rebuttal\": 4,\n}\n\nsubmission = []\nfor sample_idx, sample in enumerate(test_samples):\n    preds = sample[\"preds\"]\n    offset_mapping = sample[\"offset_mapping\"]\n    sample_id = sample[\"id\"]\n    sample_text = sample[\"text\"]\n    sample_input_ids = sample[\"input_ids\"]\n    sample_pred_scores = sample[\"pred_scores\"]\n    sample_preds = []\n\n    if len(preds) < len(offset_mapping):\n        preds = preds + [\"O\"] * (len(offset_mapping) - len(preds))\n        sample_pred_scores = sample_pred_scores + [0] * (len(offset_mapping) - len(sample_pred_scores))\n    \n    idx = 0\n    phrase_preds = []\n    while idx < len(offset_mapping):\n        start, _ = offset_mapping[idx]\n        if preds[idx] != \"O\":\n            label = preds[idx][2:]\n        else:\n            label = \"O\"\n        phrase_scores = []\n        phrase_scores.append(sample_pred_scores[idx])\n        idx += 1\n        while idx < len(offset_mapping):\n            if label == \"O\":\n                matching_label = \"O\"\n            else:\n                matching_label = f\"I-{label}\"\n            if preds[idx] == matching_label:\n                _, end = offset_mapping[idx]\n                phrase_scores.append(sample_pred_scores[idx])\n                idx += 1\n            else:\n                break\n        if \"end\" in locals():\n            phrase = sample_text[start:end]\n            phrase_preds.append((phrase, start, end, label, phrase_scores))\n\n    temp_df = []\n    for phrase_idx, (phrase, start, end, label, phrase_scores) in enumerate(phrase_preds):\n        word_start = len(sample_text[:start].split())\n        word_end = word_start + len(sample_text[start:end].split())\n        word_end = min(word_end, len(sample_text.split()))\n        ps = \" \".join([str(x) for x in range(word_start, word_end)])\n        if label != \"O\":\n            if sum(phrase_scores) / len(phrase_scores) >= proba_thresh[label]:\n                if len(ps.split()) >= min_thresh[label]:\n                    temp_df.append((sample_id, phrase, ps))\n    \n    temp_df = pd.DataFrame(temp_df, columns=[\"id\", \"class\", \"predictionstring\"])\n    submission.append(temp_df)\n\nsubmission = pd.concat(submission).reset_index(drop=True)\nsubmission = link_evidence(submission)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:45:24.080879Z","iopub.execute_input":"2022-04-28T20:45:24.081140Z","iopub.status.idle":"2022-04-28T20:45:24.138604Z","shell.execute_reply.started":"2022-04-28T20:45:24.081111Z","shell.execute_reply":"2022-04-28T20:45:24.137910Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"submission.rename(columns={\"class\": \"text\"}, inplace=True)\ntemp = submission['predictionstring'].values\ntemp = list(map(str, temp))\nsubmission['start'] = [int(x.split(' ')[0]) for x in temp]\nsubmission = submission[[\"id\", \"text\", \"predictionstring\"]]","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:45:24.956716Z","iopub.execute_input":"2022-04-28T20:45:24.957261Z","iopub.status.idle":"2022-04-28T20:45:24.967013Z","shell.execute_reply.started":"2022-04-28T20:45:24.957223Z","shell.execute_reply":"2022-04-28T20:45:24.966306Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"code","source":"submission","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:45:26.289693Z","iopub.execute_input":"2022-04-28T20:45:26.290237Z","iopub.status.idle":"2022-04-28T20:45:26.303185Z","shell.execute_reply.started":"2022-04-28T20:45:26.290198Z","shell.execute_reply":"2022-04-28T20:45:26.302447Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"# class sub_Dataset(torch.utils.data.Dataset):\n\n#     def __init__(self, df):\n\n#         self.texts = [tokenizer(text, \n#                                padding='max_length', max_length = 256, truncation=True,\n#                                 return_tensors=\"pt\") for text in df['text']]\n#         self.prev = [one_hot for one_hot in df['one_hot']]\n\n#     def get_batch_texts(self, idx):\n#         # Fetch a batch of inputs\n#         return self.texts[idx]\n    \n#     def get_batch_prev(self, idx):\n#         return self.prev[idx]\n    \n#     def __getitem__(self, idx):\n\n#         batch_texts = self.get_batch_texts(idx)\n#         batch_prev = self.get_batch_prev(idx)\n\n#         return batch_texts, batch_prev","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:06:30.228897Z","iopub.execute_input":"2022-04-28T20:06:30.229150Z","iopub.status.idle":"2022-04-28T20:06:30.236272Z","shell.execute_reply.started":"2022-04-28T20:06:30.229122Z","shell.execute_reply":"2022-04-28T20:06:30.235403Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"labels = {'Lead':0,\n          'Position':1,\n          'Claim':2,\n          'Concluding Statement':3,\n          'Evidence':4,\n          'Counterclaim':5,\n          'Rebuttal':6\n          }\nprev_label = {'Lead':0,\n          'Position':1,\n          'Claim':2,\n          'Concluding Statement':3,\n          'Evidence':4,\n          'Counterclaim':5,\n          'Rebuttal':6,\n          'Start':7\n          }","metadata":{"execution":{"iopub.status.busy":"2022-04-28T20:41:38.062052Z","iopub.execute_input":"2022-04-28T20:41:38.062392Z","iopub.status.idle":"2022-04-28T20:41:38.073012Z","shell.execute_reply.started":"2022-04-28T20:41:38.062356Z","shell.execute_reply":"2022-04-28T20:41:38.071372Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"# sequence = submission['predictionstring']\n# sequence = sequence.apply(lambda x: x.split())\n# sequence = sequence.apply(lambda x: list(map(int, x)))\n# sequence\n# submission_new = submission\n# start, end = [],[]\n# for i in range(len(sequence)):\n#     start.append(sequence[i][0])\n#     end.append(sequence[i][-1])\n# submission_new['Start'] = start\n# submission_new['End'] = end\n\n# prev = [7]\n\n# for i in range(1, len(sequence)):\n#     if submission_new['End'][i-1] < submission_new['Start'][i]:\n#         prev.append(prev_label[submission['discourse_type'][i-1]])\n#     else:\n#         prev.append(7)\n\n# df_new['one_hot'] = prev","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('../input/bigbird-roberta-base/google--bigbird-roberta-base.main.5a145f7852cba9bd431386a58137bf8a29903b90/')\n\nclass BertClassifier(nn.Module):\n\n    def __init__(self, dropout=0.5):\n\n        super(BertClassifier, self).__init__()\n        \n        self.bert = BigBirdModel.from_pretrained('../input/bigbird-roberta-base/google--bigbird-roberta-base.main.5a145f7852cba9bd431386a58137bf8a29903b90/')\n        #self.bert = BertModel.from_pretrained('bert-base-cased')\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(768, 7)\n        self.linear_2 = nn.Linear(15,7)\n        self.relu = nn.ReLU()\n\n    def forward(self, input_id, mask, prev_identity):\n        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n        dropout_output = self.dropout(pooled_output)\n        linear_output = self.linear(dropout_output)\n        linear_output = torch.cat((linear_output, prev_identity), 1)\n        linear_output = self.linear_2(linear_output)\n        final_layer = self.relu(linear_output)\n\n        return final_layer\n    \nmodel = BertClassifier()\nmodel.load_state_dict(torch.load('../input/c-bigbird-trained/bigbird-roberta-base'))","metadata":{"execution":{"iopub.status.busy":"2022-04-28T19:53:46.117172Z","iopub.execute_input":"2022-04-28T19:53:46.117643Z","iopub.status.idle":"2022-04-28T19:54:06.819099Z","shell.execute_reply.started":"2022-04-28T19:53:46.117603Z","shell.execute_reply":"2022-04-28T19:54:06.818414Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# test = sub_Dataset(submission)\n# test_dataloader = torch.utils.data.DataLoader(test, batch_size=2)\n\ndef test_data(test,temp):\n    test_new = {}\n    test_new['text'] = tokenizer(test['text'], padding='max_length', max_length = 256, truncation=True,return_tensors=\"pt\")\n    if test['id'] == temp[1]:\n        test_new['prev'] = torch.Tensor([temp[0]]).to(torch.int64)\n    else: \n        test_new['prev'] = torch.Tensor([7]).to(torch.int64)\n    test_new['id'] = test['id']\n    return test_new\n\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\"cuda\" if use_cuda else \"cpu\")\n\nif use_cuda:\n    model = model.cuda()\n    \n    temp = [torch.Tensor([1]).to(torch.int64),-1]\n    output_label = []\n    with torch.no_grad():\n\n        for _, data in submission.iterrows():\n            \n            test_input = test_data(data,temp)\n            mask = test_input['text']['attention_mask'].to(device)\n            input_id = test_input['text']['input_ids'].squeeze(1).to(device)\n            prev = test_input['prev']\n            prev = torch.nn.functional.one_hot(prev, num_classes=8).to(device)\n            \n            output = model(input_id, mask, prev)\n            label = output.argmax(dim=1)\n            \n            list_of_key = list(prev_label.keys())\n            list_of_value = list(prev_label.values())\n            \n            position = list_of_value.index(label)\n            label = list_of_key[position]\n            \n            output_label.append(label)\n            temp = [position, test_input['id']]\n\nsubmission['class'] = output_label\nsubmission = submission[['id', 'class', 'predictionstring']]\nsubmission","metadata":{"execution":{"iopub.status.busy":"2022-04-28T21:09:34.613107Z","iopub.execute_input":"2022-04-28T21:09:34.613387Z","iopub.status.idle":"2022-04-28T21:09:35.394622Z","shell.execute_reply.started":"2022-04-28T21:09:34.613357Z","shell.execute_reply":"2022-04-28T21:09:35.393729Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"submission.to_csv(\"submission.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-04-28T21:20:49.832611Z","iopub.execute_input":"2022-04-28T21:20:49.833155Z","iopub.status.idle":"2022-04-28T21:20:49.840745Z","shell.execute_reply.started":"2022-04-28T21:20:49.833117Z","shell.execute_reply":"2022-04-28T21:20:49.839987Z"},"trusted":true},"execution_count":54,"outputs":[]}]}